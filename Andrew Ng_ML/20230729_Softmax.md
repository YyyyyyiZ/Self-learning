## Softmax Regresssion

### Cost function

$$
loss(a_1,..., a_N, y)=
\begin{cases}
-loga_1,&\text{if $y=1$}\\
-loga_2,&\text{if $y=2$}\\
\qquad \vdots \\
-loga_N,&\text{if $y=N$}
\end{cases}
$$

### Softmax Layer

```python
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense

# version 1
model = Sequential([
    Dense(units = 25, activation = 'relu')
    Dense(units = 15, activation = 'relu')
    Dense(units = 10, activation = 'softmax')
])

from tensorflow.keras.losses import SparseCategoricalCrossentropy

model.compile(loss = SparseCategoricalCrossentropy())
model.fit(X, Y, epochs = 10)
```

### Improved Implementation

```python
# version 2
model = Sequential([
    Dense(units = 25, activation = 'relu')
    Dense(units = 15, activation = 'relu')
    Dense(units = 10, activation = 'linear')
])

from tensorflow.keras.losses import SparseCategoricalCrossentropy

model.compile(loss = SparseCategoricalCrossentropy(from_logits = True))
model.fit(X, Y, epochs = 10)
```

### Multi-label Classification

### Adam Algorithm

* Adam—adaptive moment estimation
	* If $w_j$ or $b$ keeps moving in same direction, incease $\alpha_j$
	* If $w_j$ or $b$ keeps oscillating, reduce $\alpha_j$

```python
from tensorflow.keras.losses import SparseCategoricalCrossentropy

model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-3), 
              loss = SparseCategoricalCrossentropy(from_logits = True))
model.fit(X, Y, epochs = 10)
```

### Additional Layer Types

* convolutional layer
	* each neuron only looks at part of the previous layer’s inputs
	* why
		* faster computation
		* need less training data (less prone to overfitting)

### Derivatives

```python
import sympy

J, w = sympy.symbols('J, w')
J = w**2
dJ_dw = sympy.diff(J, w)

dJ_dw.subs([(w, 2)])
```

