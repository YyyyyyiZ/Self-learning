## Overview

* NN
	* inference(prrediction)
	* training
* Practical advice for building machine learning systems
* Decision Trees

## NN

#### History

* Origins: algorithms that try to mimic the brain
* Used in the 1980’s and early 1990’s
* Fell out of favor in the late 1990’s
* Resurgence from around 2005
* Speech  -> Images -> text(NLP) -> ……

#### Neurons in the Brain

> ==loose== analogy

> don’t take bilogolical motivations too seriously

<img src="C:/Users/Lenovo/AppData/Roaming/Typora/typora-user-images/image-20230722105319027.png" alt="image-20230722105319027" style="zoom: 33%;" />

#### Application—CV

#### NN Layer

<img src="C:/Users/Lenovo/AppData/Roaming/Typora/typora-user-images/image-20230722113709757.png" alt="image-20230722113709757" style="zoom: 25%;" />

#### Forward Propagation—for Prediction

#### Back Propagation—for Learning

#### Matrix

<img src="C:/Users/Lenovo/AppData/Roaming/Typora/typora-user-images/image-20230722120026287.png" alt="image-20230722120026287" style="zoom: 33%;" />

## AGI—Artifical General Intelligence

### AI

* ANI—artificial narrow intelligence
	* smart speaker, self-driving car, web search, AI in farming and factories
* AGI—artificial general intelligence
	* Do anything a human can do

### “one learning algorithm” hypothesis

* most intelligence can derive from one or a small handfull of algorithms

## Forward Prop in NumPy

```python
def dense(A_in, W, B, g):
    Z = np.matmul(A_in, W) + B
    A_out = g(Z)
    return A_out

def sequential(X): # X一行为一个样本观测
    A1 = dense(X, W1, b1)
    A2 = dense(A1, W2, b2)
    A3 = dense(A2, W3, b3)
    A4 = dense(A3, W4, b4)
    f_x = A4
    return f_x
```

## Implementation of NN

1. define the model

```python
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense

model = Sequential([
    Dense(units = 25, activation = 'sigmoid')
    Dense(units = 15, activation = 'sigmoid')
    Dense(units = 1, activation = 'sigmoid')
])
```

2. define loss and cost functions

```python
from tensorflow.keras.losses import BinaryCrossentropy

model.compile(loss  = BinartCrossentropy())

from tensorflow.keras.losses import MeanSquaredError
model.compile(loss = MeanSquaredError())
model.fit(X, Y, epochs = 100)
# fit
logit = model(X)
# predict
f_x = tf.nn.sigmoid(logit)
```

3. gradient descent

```python
# compute derivatives for GD using back propagation  BP
model.fit(X, y, epochs = 100)
```

## Activation

> Don’t use linear activation function in hidden layer

```python
# binary classification
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# y both positive and negative
def sigmoid_de(x):
    tmp = sigmoid(x)
    return tmp * (1 - tmp)

# only positive
def relu(x):
    return np.maximum(x, 0)


def relu_de(x):
    return np.where(x > 0, 1, 0)
```

## Softmax
