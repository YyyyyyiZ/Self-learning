# Case Studies

## LeNet-5[^1]

![LeNet-5](https://img-blog.csdnimg.cn/img_convert/ff7818be02f52eb088b1b114f664cf2b.png)

[^1]:LeCun et al., 1998. Gradient-based learning applied to document recognition

## AlexNet[^2]

![image-20230806084127112](C:/Users/Lenovo/AppData/Roaming/Typora/typora-user-images/image-20230806084127112.png)

* Similar to LeNet-5, but much larger, ~60m parameters
* ReLU as activation function

[^2]: Krizhevsky et al., 2012. ImageNet classification with deep convolutional neural networks

## VGG-16[^3]

![](https://img-blog.csdnimg.cn/20210310114812936.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ1OTk4MDQx,size_16,color_FFFFFF,t_70#pic_center)

* large network ~138m parameters
* uniform architecture

[^3]: Simonyan & Zisserman 2015. Very deep convolutional networks for large-scale image recognition

## Residual Networks[^4]

### Residual Block

![](https://pic4.zhimg.com/80/v2-252e6d9979a2a91c2d3033b9b73eb69f_720w.webp)

### ResNet

![](https://pic2.zhimg.com/80/v2-7cb9c03871ab1faa7ca23199ac403bd9_720w.webp)

[^4]: He et al., 2015. Deep residual networks for image recognition

### Why ResNet

* Degradation Problem
	* not overfitting, but hard to train

* Identity mapping

## 1×1 Convolution[^5]

> network in network

* non-trivial operation
* shrink the channels

[^5]: Lin et al., 2013. Network in network

## Inception Network[^6]

> Inception—instead of choosing what filter size you want in Conv Layer or even do you want a convolutional layer or a pooling layer? Let’s do them all.

### Motivation

<img src="C:/Users/Lenovo/AppData/Roaming/Typora/typora-user-images/image-20230806092718579.png" alt="image-20230806092718579" style="zoom:50%;" />

### computational cost

#### Originial Implementation

<img src="C:/Users/Lenovo/AppData/Roaming/Typora/typora-user-images/image-20230806093400459.png" alt="image-20230806093400459" style="zoom: 60%;" />

#### Using 1×1 Convolution

> bottleneck layer

<img src="C:/Users/Lenovo/AppData/Roaming/Typora/typora-user-images/image-20230806093249012.png" alt="image-20230806093249012" style="zoom:50%;" />

* shrink the number of channels

### Inception Module

![Inception Module](https://img-blog.csdnimg.cn/20200927205914238.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Zlcm1vbnRf,size_16,color_FFFFFF,t_70)

[^6]: Szegedy et al. 2014. Going deeper with convolutions

## MobileNet[^7]

### Depthwise Separable Convolution

> depthwise+pointwise

<img src="C:/Users/Lenovo/AppData/Roaming/Typora/typora-user-images/image-20230806095415945.png" alt="image-20230806095415945" style="zoom:50%;" />

* Low computational cost at deployment
* normal convolution~2160; depthwise separable convolution~432+240

[^7]: Howard et al. 2017, MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications

### MobileNet Architecture[^8]

#### V1

<img src="C:/Users/Lenovo/AppData/Roaming/Typora/typora-user-images/image-20230806100608968.png" alt="image-20230806100608968" style="zoom: 50%;" />

#### V2

<img src="C:/Users/Lenovo/AppData/Roaming/Typora/typora-user-images/image-20230806100629436.png" alt="image-20230806100629436" style="zoom: 60%;" />

<img src="C:/Users/Lenovo/AppData/Roaming/Typora/typora-user-images/image-20230806100713077.png" alt="image-20230806100713077" style="zoom: 50%;" />

[^8]: Sandler et al. 2019, MobileNetV2: Inverted Residuals and Linear Bottlenecks

## EfficientNet[^9]

* achieve a balance among r, d and w
	* resolution
	* depth
	* width

[^9]: Tan and Le, 2019, EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks

# Implementation

### Open source code

* use architectures of networks published in the literature
* Use open source implementations if possible
* use pretrained models and fine-tune on your dataset

### Transfer Learning

> choose the layers to train depending on the amount of your training data

### Data Augmentation

* mirroring
* random cropping
* color shifting
	* PCA color augmentation

### Hand Engineering

> useful when data is insufficient
